{"componentChunkName":"component---src-pages-automation-pre-requisites-index-mdx","path":"/automation/pre-requisites/","result":{"pageContext":{"frontmatter":{"title":"Prerequisites","weight":200},"relativePagePath":"/automation/pre-requisites/index.mdx","titleType":"page","MdxNode":{"id":"0c5a62b4-aca6-5f87-b194-22a72f91ed10","children":[],"parent":"61096e0e-a779-58f4-bcc6-fddcb0dc38cf","internal":{"content":"---\ntitle: Prerequisites\nweight: 200\n---\n- \n# make sure there is a space after the - so that the TOC is generated\n{:toc}\n\n## Install\n\n### NFS\nThe persistent volumes used by the different Cloud Pak for Automation components in the following chapters are relying on NFS. Before starting the install of any component, it is thus required to set-up an NFS server. An example for how to set-up and verify an NFS server for Redhat 7 can be found [here](https://linuxconfig.org/quick-nfs-server-configuration-on-redhat-7-linux).\n\n### Helm\n\nThe following instructions are extracted from [here](https://blog.openshift.com/getting-started-helm-openshift/).\n\n- Download `helm` binaries and install the client only:\n```\nwget https://get.helm.sh/helm-v2.12.2-linux-386.tar.gz\ntar -zxvf helm-v2.12.2-linux-386.tar.gz\nmv linux-386/helm /usr/local/bin/\nhelm init --client-only\nhelm version\n```\n\n- Create an openshift project where the Helm `tiller` (the server side) will be installed:\n```\noc new-project tiller\noc project tiller\nexport TILLER_NAMESPACE=tiller\n```\nYou can add the `export TILLER_NAMESPACE=tiller` to your `~/.bash_profile` for instance to avoid exporting in each session.\n\n- Install the `tiller`:\n\nUse the same version of the client in the following command line to have the same version of client and tiller.\n```\noc process -f https://github.com/openshift/origin/raw/master/examples/helm/tiller-template.yaml -p TILLER_NAMESPACE=\"${TILLER_NAMESPACE}\" -p HELM_VERSION=v2.12.2 | oc create -f -\noc get pods\n# Check pods are running\noc rollout status deployment tiller\nhelm init\nhelm version\n```\n\n## Prepare\n\n### Logging-in to your cluster\n\n#### IBM Cloud OpenShift cluster\nStart by loging in to IBM Cloud with the `ibmcloud login` or `ibmcloud login --sso` command, then select your cluster and login to it.\n```\nibmcloud oc cluster-config --cluster <your-cluster-name>\noc login \n```\n\n#### On-prem OpenShift cluster\nLogin directly to your cluster:\n```\noc login -u admin -p admin https://<your-cluster-url>/\n```\n\n### Accessing the Docker registry\n\n#### IBM Cloud OpenShift cluster\nTo expose the `docker-registry.default.svc`, open a command window, login to OpenShift and run the following command:\n```\nkubectl -n default port-forward svc/docker-registry 5000:5000 &\n```\nThis is exposing port 5000 on the boot node (wherever this is run). You need to leave the command window open or else the port-forwarding will stop. Be aware of the potential timeout of port forwarding during the images push.\n\n#### On-prem OpenShift cluster\nTo prepare Docker access, edit the `/etc/docker/daemon.json` Docker daemon configuration file to include the `\"insecure-registries\"` property, as shown on the example below:\n``` \n{\n  \"insecure-registries\" : [\"docker-registry-default.apps-cp4a-res.rtp.raleigh.ibm.com\"]\n}\n```\nRestart docker daemon:\n```\nsystemctl restart docker\n```\n#### OpenShift cluster accessing IBM Cloud image registry\nYou need to use a `secret` containing credentials to access IBM Cloud registry. To create this secret you need to generate a key from your IBM Cloud entitlment to access Cloud Pak for Automation docker images.\n```\noc create secret docker-registry cp-entitlement --docker-server=cp.icr.io --docker-username=ekey --docker-password=<GENERATED_KEY_FROM_IBM_CLOUD_ENTATLMENT> --docker-email=unused\n```\n\n","type":"Mdx","contentDigest":"27987f1c8a5e84710e411231f423a540","counter":226,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"Prerequisites","weight":200},"exports":{},"rawBody":"---\ntitle: Prerequisites\nweight: 200\n---\n- \n# make sure there is a space after the - so that the TOC is generated\n{:toc}\n\n## Install\n\n### NFS\nThe persistent volumes used by the different Cloud Pak for Automation components in the following chapters are relying on NFS. Before starting the install of any component, it is thus required to set-up an NFS server. An example for how to set-up and verify an NFS server for Redhat 7 can be found [here](https://linuxconfig.org/quick-nfs-server-configuration-on-redhat-7-linux).\n\n### Helm\n\nThe following instructions are extracted from [here](https://blog.openshift.com/getting-started-helm-openshift/).\n\n- Download `helm` binaries and install the client only:\n```\nwget https://get.helm.sh/helm-v2.12.2-linux-386.tar.gz\ntar -zxvf helm-v2.12.2-linux-386.tar.gz\nmv linux-386/helm /usr/local/bin/\nhelm init --client-only\nhelm version\n```\n\n- Create an openshift project where the Helm `tiller` (the server side) will be installed:\n```\noc new-project tiller\noc project tiller\nexport TILLER_NAMESPACE=tiller\n```\nYou can add the `export TILLER_NAMESPACE=tiller` to your `~/.bash_profile` for instance to avoid exporting in each session.\n\n- Install the `tiller`:\n\nUse the same version of the client in the following command line to have the same version of client and tiller.\n```\noc process -f https://github.com/openshift/origin/raw/master/examples/helm/tiller-template.yaml -p TILLER_NAMESPACE=\"${TILLER_NAMESPACE}\" -p HELM_VERSION=v2.12.2 | oc create -f -\noc get pods\n# Check pods are running\noc rollout status deployment tiller\nhelm init\nhelm version\n```\n\n## Prepare\n\n### Logging-in to your cluster\n\n#### IBM Cloud OpenShift cluster\nStart by loging in to IBM Cloud with the `ibmcloud login` or `ibmcloud login --sso` command, then select your cluster and login to it.\n```\nibmcloud oc cluster-config --cluster <your-cluster-name>\noc login \n```\n\n#### On-prem OpenShift cluster\nLogin directly to your cluster:\n```\noc login -u admin -p admin https://<your-cluster-url>/\n```\n\n### Accessing the Docker registry\n\n#### IBM Cloud OpenShift cluster\nTo expose the `docker-registry.default.svc`, open a command window, login to OpenShift and run the following command:\n```\nkubectl -n default port-forward svc/docker-registry 5000:5000 &\n```\nThis is exposing port 5000 on the boot node (wherever this is run). You need to leave the command window open or else the port-forwarding will stop. Be aware of the potential timeout of port forwarding during the images push.\n\n#### On-prem OpenShift cluster\nTo prepare Docker access, edit the `/etc/docker/daemon.json` Docker daemon configuration file to include the `\"insecure-registries\"` property, as shown on the example below:\n``` \n{\n  \"insecure-registries\" : [\"docker-registry-default.apps-cp4a-res.rtp.raleigh.ibm.com\"]\n}\n```\nRestart docker daemon:\n```\nsystemctl restart docker\n```\n#### OpenShift cluster accessing IBM Cloud image registry\nYou need to use a `secret` containing credentials to access IBM Cloud registry. To create this secret you need to generate a key from your IBM Cloud entitlment to access Cloud Pak for Automation docker images.\n```\noc create secret docker-registry cp-entitlement --docker-server=cp.icr.io --docker-username=ekey --docker-password=<GENERATED_KEY_FROM_IBM_CLOUD_ENTATLMENT> --docker-email=unused\n```\n\n","fileAbsolutePath":"/home/travis/build/ibm-cloud-architecture/cloudpak8s/src/pages/automation/pre-requisites/index.mdx"}}}}