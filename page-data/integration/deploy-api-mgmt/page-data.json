{"componentChunkName":"component---src-pages-integration-deploy-api-mgmt-index-mdx","path":"/integration/deploy-api-mgmt/","result":{"pageContext":{"frontmatter":{"title":"deploy-api-mgmt","weight":600},"relativePagePath":"/integration/deploy-api-mgmt/index.mdx","titleType":"page","MdxNode":{"id":"c0ebc0a2-3b45-5a85-9668-616341f45971","children":[],"parent":"338348f5-d973-5900-8ac7-69d3145cf414","internal":{"content":"---\ntitle: deploy-api-mgmt\nweight: 600\n---\n\nThis page contains guidance on how to configure the APIC release for both on-prem and ROKS.\n\n### Prepare endpoints\n\nWe have to define the endpoint for each of the APIC subsystems. We can \"construct\" the endpoints by adding descriptive \"prefixes\" to the proxy URL. In the sample described here, the proxy URL was `icp-proxy.icp4i-6550a99fb8cff23207ccecc2183787a9-0001.us-east.containers.appdomain.cloud` so we defined the endpoints as follows:\n\nManagement - all endpoints:\n```\nmgmt.icp-proxy.icp4i-6550a99fb8cff23207ccecc2183787a9-0001.us-east.containers.appdomain.cloud\n```\n\nGateway - API:\n```\ngw.icp-proxy.icp4i-6550a99fb8cff23207ccecc2183787a9-0001.us-east.containers.appdomain.cloud\n```\n\nGateway - service:\n```\ngwd.icp-proxy.icp4i-6550a99fb8cff23207ccecc2183787a9-0001.us-east.containers.appdomain.cloud\n```\n\nAnalytics - ingestion:\n```\nai.icp-proxy.icp4i-6550a99fb8cff23207ccecc2183787a9-0001.us-east.containers.appdomain.cloud\n```\n\nAnalytics - client:\n```\nac.icp-proxy.icp4i-6550a99fb8cff23207ccecc2183787a9-0001.us-east.containers.appdomain.cloud\n```\n\nPortal - web\n```\nportal.icp-proxy.icp4i-6550a99fb8cff23207ccecc2183787a9-0001.us-east.containers.appdomain.cloud\n```\n\nPortal - director:\n```\npadmin.icp-proxy.icp4i-6550a99fb8cff23207ccecc2183787a9-0001.us-east.containers.appdomain.cloud\n```\n\n### Obtain the pull secret\n\nTo obtain the secret for pulling the image login to the OCP CLI and run:\n```\noc get secrets -n apic\n```\nFor Offline Installs - The pull secret starts with **deployer-dockercfg**, in our case it was:\n```\ndeployer-dockercfg-7mlqd\n```\nFor Online/entitled Registry Installs - use the `ibm-entitlement-key` pull secret\n\n### Create the TLS secret.\n\nThe easiest way to accomplish this is to create the TLS Secret using the Visual Web Terminal inside of the Cloud Pak Foundation window.  To access this window do the following\n\n1. Via the Platform Navigator. Select the Hamburger menu, top left and then select **Cloud Pak Foundation**\n![](8.common-cli.png)\n2. Select the Visual Web Terminal icon.  2nd Icon from the right (looks like the box)\n![](9.cli2.png)\n3. The Visual Web Terminal will start and then once it connects to your cluster you can enter in commands.  Try to enter a command like `helm ls`.  You should see output like the following:\n![](10.visual.png)\n4. Now you can run the following command to create the TLS secret:\n```\noc create secret generic apic-ent-helm-tls --from-file=cert.pem=$HOME/.helm/cert.pem --from-file=ca.pem=$HOME/.helm/ca.pem --from-file=key.pem=$HOME/.helm/key.pem -n apic\n```\nwhere **apic-ent-helm-tls** is the name of the secret.\n\n### Increase vm.max_map_count\n\nTo check and increase `vm.max_map_count` we would need an *ssh* access to each of the cluster nodes.\n\nThe alternative is to create a DaemonSet which will do that for us. Prepare the yaml file with the following content:\n```yaml\napiVersion: extensions/v1beta1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: sysctl-conf\n  name: sysctl-conf\n  namespace: kube-system\nspec:\n  template:\n    metadata:\n      labels:\n        k8s-app: sysctl-conf\n    spec:\n      containers:\n      - command:\n        - sh\n        - -c\n        - sysctl -w vm.max_map_count=262144 && while true; do sleep 86400; done\n        image: busybox:1.26.2\n        name: sysctl-conf\n        resources:\n          limits:\n            cpu: 10m\n            memory: 50Mi\n          requests:\n            cpu: 10m\n            memory: 50Mi\n        securityContext:\n          privileged: true\n      terminationGracePeriodSeconds: 1\n```\n\nand run apply it with:\n\n```\noc apply -f sysctl-conf.yaml\n```\n*Note* if you have done something similar for eventstreams, note that the required value of vm.max_map_count is higher than what was required\n\n### Storage class\n\nThe **block storage class** is needed for APIC.\nYou can obtain the class names with\n```\noc get storageclass\n```\n\nThe follwing classes are available on ROKS:\n```\nNAME                          PROVISIONER         AGE\ndefault                       ibm.io/ibmc-file    9d\nibmc-block-bronze (default)   ibm.io/ibmc-block   9d\nibmc-block-custom             ibm.io/ibmc-block   9d\nibmc-block-gold               ibm.io/ibmc-block   9d\nibmc-block-retain-bronze      ibm.io/ibmc-block   9d\nibmc-block-retain-custom      ibm.io/ibmc-block   9d\nibmc-block-retain-gold        ibm.io/ibmc-block   9d\nibmc-block-retain-silver      ibm.io/ibmc-block   9d\nibmc-block-silver             ibm.io/ibmc-block   9d\nibmc-file-bronze              ibm.io/ibmc-file    9d\nibmc-file-custom              ibm.io/ibmc-file    9d\nibmc-file-gold                ibm.io/ibmc-file    9d\nibmc-file-retain-bronze       ibm.io/ibmc-file    9d\nibmc-file-retain-custom       ibm.io/ibmc-file    9d\nibmc-file-retain-gold         ibm.io/ibmc-file    9d\nibmc-file-retain-silver       ibm.io/ibmc-file    9d\nibmc-file-silver              ibm.io/ibmc-file    9d\n```\n\nIn our case, we decided to use `ibmc-block-gold`.  This will work with IBM Cloud based installs.  Offline Installs Require Ceph.  Other Clouds like AWS have their own block storage.  Be sure to check their documentation.\n\n\n### Create an instance\n\n- Be sure to set permissions using the following.  Make sure you have changed context to the apic project via `oc project apic`\n```\noc adm policy add-scc-to-group ibm-anyuid-scc system:serviceaccounts:apic\noc adm policy add-scc-to-group anyuid system:serviceaccounts:apic\n```\n\n- Open platform navigator and select **API Connect** / **Create instance**\n\n- Click *Continue*\n- Define the helm release name, select **apic** namespace and the local-cluster.\n\n- Enter the registry secret name, helm TLS secret name and select storage class:\n\n- Enter the management and portal endpoints:\n![Platform endpoints](/assets/img/integration/apic-roks/Snip20190909_16.png)\n\n- Scroll enter the analytics and gateway endpoints:\n![Gateway endpoints](/assets/img/integration/apic-roks/Snip20190909_17.png)\n\n- If not already, switch the view to show all parameters\n![All params](/assets/img/integration/apic-roks/Snip20190909_17c.png)\n\n- Find the *Routing Type* parameter. For running on OpenShift, the type must be **Route** instead of the default *Ingress*.\n![Routung type](/assets/img/integration/apic-roks/Snip20190909_17a.png)\n\n- For the non-production installation, you may switch the mode to **dev**\n![Mode](/assets/img/integration/apic-roks/Snip20190909_17d.png)\n\n- and the number of gateway replicas to **1**\n![Replicas](/assets/img/integration/apic-roks/Snip20190909_17b.png)\n\n- Click on **Install**, the confirmation message will appear:\n![Install](/assets/img/integration/apic-roks/Snip20190909_19.png)\n\n- You can check the status of the pods with the command:\n```\noc get pods -n apic\n```\n- When deployment is completed, all pods must be in **Running** or **Completed** state.  This entire process could take over an hour to complete.  The list of pods should look similar to this one:\n```\n### Configure APIC to work with Tracing\n\n1. Near the end of the install of APIC, a job will be created that has the name `odtracing.registration`.  This job will not complete until the Registration is completed inside of the Tracing capability.\n2. What will happen is that a request will be created inside of tracing that you need to act upon.  Navigate to the Platform Navigator and via the Hamburger menu select Tracing and then when the window pops out select the name of your tracing instance which should be called `tracing`\n![](13.tracing-nav.png)\n3. Within tracing, select the `Manage` icon from the menu.  Looks like a cog wheel.\n![](14.tracing-from-menu.png)\n4. Click on the `Registration Requests` icon.\n5. You should see a new registration request for your APIC install.  Click the `approve` link\n6. You will see a pop up window with some lines to copy to your clipboard.  Click the 2 boxes icon in the top right icon to copy the commands required.\n![](15.process-request.png)\n7. Ensuring you have an active `oc` session and in the `apic` project.  Paste the commands to the window and it will run then and finish the processing.\n8. If you are slow in doing the steps above.  It is possible you might see the `odtracing.registration` job fail.  No worries.  Once you complete the pasting of the commands to create your secret, the job will re-create itself.\n\n\n### SMTP server\n\nIn order to configure the API Connect, we need a SMTP server. If we don't have one, we can run a developer type SMTP Service.  `mailtrap.io` is a good one.\n\n\n### Configuring the API Connect\n\n- You can access your new install by starting from the Platform Navigator\n\n- Select IBM Cloud Private user, default username and password in this case are admin/admin\n![Login CMC](/assets/img/integration/apic-roks/Snip20190910_32.png)\n\n- Under **Resources/Notifications** define the SMTP server\n![SMTP](/assets/img/integration/apic-roks/Snip20190910_51.png)\n\n- For our Mailtrap server enter ClusterIP address and port:\n![SMTP](/assets/img/integration/apic-roks/Snip20190910_53.png)\n\n- Under **Settings/Notifications** edit the sender email server:\n![SMTP](/assets/img/integration/apic-roks/Snip20190910_56.png)\n\n- And select the SMTP server defined under resources:\n![email](/assets/img/integration/apic-roks/Snip20190910_57.png)\n\n- Start with the **Topology** configuration\n![Topology](/assets/img/integration/apic-roks/Snip20190910_34.png)\n\n- Register service:\n![Register Service](/assets/img/integration/apic-roks/Snip20190910_35.png)\n\n- Start with the Gateway, select the version that you defined under the Helm release properties when you started creating the instance. In our case it was V5 compatible version:\n![Gateway](/assets/img/integration/apic-roks/Snip20190910_36.png)\n\n- Give some name to the service (e.g. **gateway1**) enter the **endpoints** and click on **Save**:\n![Gateway](/assets/img/integration/apic-roks/Snip20190910_38.png)\n\n- The confirmation message should appear:\n![Gateway](/assets/img/integration/apic-roks/Snip20190910_40.png)\n\n- Click on *Register service* again and select Analytics:\n![Analytics](/assets/img/integration/apic-roks/Snip20190910_43.png)\n\n- Give some name to the service, enter Management endpoint (the one that you defined for **analytics client**) and click **Save**\n![Analytics](/assets/img/integration/apic-roks/Snip20190910_44.png)\n\n- The confirmation appears:\n![Analytics](/assets/img/integration/apic-roks/Snip20190910_46.png)\n\n- Repeat the same with portal:\n![Portal](/assets/img/integration/apic-roks/Snip20190910_48.png)\n\n- The confirmation appears again:\n![Portal](/assets/img/integration/apic-roks/Snip20190910_62.png)\n\n- Click on **Associate Analytics Service** to associate analytics with the gateway:\n![Associate analytics](/assets/img/integration/apic-roks/Snip20190910_63.png)\n\n- Select the analytics service:\n![Associate analytics](/assets/img/integration/apic-roks/Snip20190910_64.png)\n\n- Click on **Provider organizations** and add new organization:\n![ProvOrg](/assets/img/integration/apic-roks/Snip20190910_66.png)\n\n- Give some name to the organization:\n![ProvOrg](/assets/img/integration/apic-roks/Snip20190910_67.png)\n\n- Define the owner\n![ProvOrg](/assets/img/integration/apic-roks/Snip20190910_68.png)\n\n- After you submit the organization will appear on the list:\n![ProvOrg](/assets/img/integration/apic-roks/Snip20190910_69.png)\n\n- Navigate to the API Manager, in our case the endpoint was:\nhttps://mgmt.icp-proxy.icp4i-6550a99fb8cff23207ccecc2183787a9-0001.us-east.containers.appdomain.cloud/manage\n\n- Login as the owner (defined in the previous step), the API Manager page should open:\n![API Mgr](/assets/img/integration/apic-roks/Snip20190910_70.png)\n\n- You can navigate to the catalog:\n![Sandbox](/assets/img/integration/apic-roks/Snip20190910_71.png)\n\n- and create portal\n![Create portal](/assets/img/integration/apic-roks/Snip20190910_73.png)\n\n- You can also assign the gateway to the catalog\n![Catalog](/assets/img/integration/apic-roks/Snip20190910_79.png)\n\nWith that, your API Connect instance is ready for usage. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n---\n","type":"Mdx","contentDigest":"05cd48c595c4a2ece606e696e54d5aba","counter":264,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"deploy-api-mgmt","weight":600},"exports":{},"rawBody":"---\ntitle: deploy-api-mgmt\nweight: 600\n---\n\nThis page contains guidance on how to configure the APIC release for both on-prem and ROKS.\n\n### Prepare endpoints\n\nWe have to define the endpoint for each of the APIC subsystems. We can \"construct\" the endpoints by adding descriptive \"prefixes\" to the proxy URL. In the sample described here, the proxy URL was `icp-proxy.icp4i-6550a99fb8cff23207ccecc2183787a9-0001.us-east.containers.appdomain.cloud` so we defined the endpoints as follows:\n\nManagement - all endpoints:\n```\nmgmt.icp-proxy.icp4i-6550a99fb8cff23207ccecc2183787a9-0001.us-east.containers.appdomain.cloud\n```\n\nGateway - API:\n```\ngw.icp-proxy.icp4i-6550a99fb8cff23207ccecc2183787a9-0001.us-east.containers.appdomain.cloud\n```\n\nGateway - service:\n```\ngwd.icp-proxy.icp4i-6550a99fb8cff23207ccecc2183787a9-0001.us-east.containers.appdomain.cloud\n```\n\nAnalytics - ingestion:\n```\nai.icp-proxy.icp4i-6550a99fb8cff23207ccecc2183787a9-0001.us-east.containers.appdomain.cloud\n```\n\nAnalytics - client:\n```\nac.icp-proxy.icp4i-6550a99fb8cff23207ccecc2183787a9-0001.us-east.containers.appdomain.cloud\n```\n\nPortal - web\n```\nportal.icp-proxy.icp4i-6550a99fb8cff23207ccecc2183787a9-0001.us-east.containers.appdomain.cloud\n```\n\nPortal - director:\n```\npadmin.icp-proxy.icp4i-6550a99fb8cff23207ccecc2183787a9-0001.us-east.containers.appdomain.cloud\n```\n\n### Obtain the pull secret\n\nTo obtain the secret for pulling the image login to the OCP CLI and run:\n```\noc get secrets -n apic\n```\nFor Offline Installs - The pull secret starts with **deployer-dockercfg**, in our case it was:\n```\ndeployer-dockercfg-7mlqd\n```\nFor Online/entitled Registry Installs - use the `ibm-entitlement-key` pull secret\n\n### Create the TLS secret.\n\nThe easiest way to accomplish this is to create the TLS Secret using the Visual Web Terminal inside of the Cloud Pak Foundation window.  To access this window do the following\n\n1. Via the Platform Navigator. Select the Hamburger menu, top left and then select **Cloud Pak Foundation**\n![](8.common-cli.png)\n2. Select the Visual Web Terminal icon.  2nd Icon from the right (looks like the box)\n![](9.cli2.png)\n3. The Visual Web Terminal will start and then once it connects to your cluster you can enter in commands.  Try to enter a command like `helm ls`.  You should see output like the following:\n![](10.visual.png)\n4. Now you can run the following command to create the TLS secret:\n```\noc create secret generic apic-ent-helm-tls --from-file=cert.pem=$HOME/.helm/cert.pem --from-file=ca.pem=$HOME/.helm/ca.pem --from-file=key.pem=$HOME/.helm/key.pem -n apic\n```\nwhere **apic-ent-helm-tls** is the name of the secret.\n\n### Increase vm.max_map_count\n\nTo check and increase `vm.max_map_count` we would need an *ssh* access to each of the cluster nodes.\n\nThe alternative is to create a DaemonSet which will do that for us. Prepare the yaml file with the following content:\n```yaml\napiVersion: extensions/v1beta1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: sysctl-conf\n  name: sysctl-conf\n  namespace: kube-system\nspec:\n  template:\n    metadata:\n      labels:\n        k8s-app: sysctl-conf\n    spec:\n      containers:\n      - command:\n        - sh\n        - -c\n        - sysctl -w vm.max_map_count=262144 && while true; do sleep 86400; done\n        image: busybox:1.26.2\n        name: sysctl-conf\n        resources:\n          limits:\n            cpu: 10m\n            memory: 50Mi\n          requests:\n            cpu: 10m\n            memory: 50Mi\n        securityContext:\n          privileged: true\n      terminationGracePeriodSeconds: 1\n```\n\nand run apply it with:\n\n```\noc apply -f sysctl-conf.yaml\n```\n*Note* if you have done something similar for eventstreams, note that the required value of vm.max_map_count is higher than what was required\n\n### Storage class\n\nThe **block storage class** is needed for APIC.\nYou can obtain the class names with\n```\noc get storageclass\n```\n\nThe follwing classes are available on ROKS:\n```\nNAME                          PROVISIONER         AGE\ndefault                       ibm.io/ibmc-file    9d\nibmc-block-bronze (default)   ibm.io/ibmc-block   9d\nibmc-block-custom             ibm.io/ibmc-block   9d\nibmc-block-gold               ibm.io/ibmc-block   9d\nibmc-block-retain-bronze      ibm.io/ibmc-block   9d\nibmc-block-retain-custom      ibm.io/ibmc-block   9d\nibmc-block-retain-gold        ibm.io/ibmc-block   9d\nibmc-block-retain-silver      ibm.io/ibmc-block   9d\nibmc-block-silver             ibm.io/ibmc-block   9d\nibmc-file-bronze              ibm.io/ibmc-file    9d\nibmc-file-custom              ibm.io/ibmc-file    9d\nibmc-file-gold                ibm.io/ibmc-file    9d\nibmc-file-retain-bronze       ibm.io/ibmc-file    9d\nibmc-file-retain-custom       ibm.io/ibmc-file    9d\nibmc-file-retain-gold         ibm.io/ibmc-file    9d\nibmc-file-retain-silver       ibm.io/ibmc-file    9d\nibmc-file-silver              ibm.io/ibmc-file    9d\n```\n\nIn our case, we decided to use `ibmc-block-gold`.  This will work with IBM Cloud based installs.  Offline Installs Require Ceph.  Other Clouds like AWS have their own block storage.  Be sure to check their documentation.\n\n\n### Create an instance\n\n- Be sure to set permissions using the following.  Make sure you have changed context to the apic project via `oc project apic`\n```\noc adm policy add-scc-to-group ibm-anyuid-scc system:serviceaccounts:apic\noc adm policy add-scc-to-group anyuid system:serviceaccounts:apic\n```\n\n- Open platform navigator and select **API Connect** / **Create instance**\n\n- Click *Continue*\n- Define the helm release name, select **apic** namespace and the local-cluster.\n\n- Enter the registry secret name, helm TLS secret name and select storage class:\n\n- Enter the management and portal endpoints:\n![Platform endpoints](/assets/img/integration/apic-roks/Snip20190909_16.png)\n\n- Scroll enter the analytics and gateway endpoints:\n![Gateway endpoints](/assets/img/integration/apic-roks/Snip20190909_17.png)\n\n- If not already, switch the view to show all parameters\n![All params](/assets/img/integration/apic-roks/Snip20190909_17c.png)\n\n- Find the *Routing Type* parameter. For running on OpenShift, the type must be **Route** instead of the default *Ingress*.\n![Routung type](/assets/img/integration/apic-roks/Snip20190909_17a.png)\n\n- For the non-production installation, you may switch the mode to **dev**\n![Mode](/assets/img/integration/apic-roks/Snip20190909_17d.png)\n\n- and the number of gateway replicas to **1**\n![Replicas](/assets/img/integration/apic-roks/Snip20190909_17b.png)\n\n- Click on **Install**, the confirmation message will appear:\n![Install](/assets/img/integration/apic-roks/Snip20190909_19.png)\n\n- You can check the status of the pods with the command:\n```\noc get pods -n apic\n```\n- When deployment is completed, all pods must be in **Running** or **Completed** state.  This entire process could take over an hour to complete.  The list of pods should look similar to this one:\n```\n### Configure APIC to work with Tracing\n\n1. Near the end of the install of APIC, a job will be created that has the name `odtracing.registration`.  This job will not complete until the Registration is completed inside of the Tracing capability.\n2. What will happen is that a request will be created inside of tracing that you need to act upon.  Navigate to the Platform Navigator and via the Hamburger menu select Tracing and then when the window pops out select the name of your tracing instance which should be called `tracing`\n![](13.tracing-nav.png)\n3. Within tracing, select the `Manage` icon from the menu.  Looks like a cog wheel.\n![](14.tracing-from-menu.png)\n4. Click on the `Registration Requests` icon.\n5. You should see a new registration request for your APIC install.  Click the `approve` link\n6. You will see a pop up window with some lines to copy to your clipboard.  Click the 2 boxes icon in the top right icon to copy the commands required.\n![](15.process-request.png)\n7. Ensuring you have an active `oc` session and in the `apic` project.  Paste the commands to the window and it will run then and finish the processing.\n8. If you are slow in doing the steps above.  It is possible you might see the `odtracing.registration` job fail.  No worries.  Once you complete the pasting of the commands to create your secret, the job will re-create itself.\n\n\n### SMTP server\n\nIn order to configure the API Connect, we need a SMTP server. If we don't have one, we can run a developer type SMTP Service.  `mailtrap.io` is a good one.\n\n\n### Configuring the API Connect\n\n- You can access your new install by starting from the Platform Navigator\n\n- Select IBM Cloud Private user, default username and password in this case are admin/admin\n![Login CMC](/assets/img/integration/apic-roks/Snip20190910_32.png)\n\n- Under **Resources/Notifications** define the SMTP server\n![SMTP](/assets/img/integration/apic-roks/Snip20190910_51.png)\n\n- For our Mailtrap server enter ClusterIP address and port:\n![SMTP](/assets/img/integration/apic-roks/Snip20190910_53.png)\n\n- Under **Settings/Notifications** edit the sender email server:\n![SMTP](/assets/img/integration/apic-roks/Snip20190910_56.png)\n\n- And select the SMTP server defined under resources:\n![email](/assets/img/integration/apic-roks/Snip20190910_57.png)\n\n- Start with the **Topology** configuration\n![Topology](/assets/img/integration/apic-roks/Snip20190910_34.png)\n\n- Register service:\n![Register Service](/assets/img/integration/apic-roks/Snip20190910_35.png)\n\n- Start with the Gateway, select the version that you defined under the Helm release properties when you started creating the instance. In our case it was V5 compatible version:\n![Gateway](/assets/img/integration/apic-roks/Snip20190910_36.png)\n\n- Give some name to the service (e.g. **gateway1**) enter the **endpoints** and click on **Save**:\n![Gateway](/assets/img/integration/apic-roks/Snip20190910_38.png)\n\n- The confirmation message should appear:\n![Gateway](/assets/img/integration/apic-roks/Snip20190910_40.png)\n\n- Click on *Register service* again and select Analytics:\n![Analytics](/assets/img/integration/apic-roks/Snip20190910_43.png)\n\n- Give some name to the service, enter Management endpoint (the one that you defined for **analytics client**) and click **Save**\n![Analytics](/assets/img/integration/apic-roks/Snip20190910_44.png)\n\n- The confirmation appears:\n![Analytics](/assets/img/integration/apic-roks/Snip20190910_46.png)\n\n- Repeat the same with portal:\n![Portal](/assets/img/integration/apic-roks/Snip20190910_48.png)\n\n- The confirmation appears again:\n![Portal](/assets/img/integration/apic-roks/Snip20190910_62.png)\n\n- Click on **Associate Analytics Service** to associate analytics with the gateway:\n![Associate analytics](/assets/img/integration/apic-roks/Snip20190910_63.png)\n\n- Select the analytics service:\n![Associate analytics](/assets/img/integration/apic-roks/Snip20190910_64.png)\n\n- Click on **Provider organizations** and add new organization:\n![ProvOrg](/assets/img/integration/apic-roks/Snip20190910_66.png)\n\n- Give some name to the organization:\n![ProvOrg](/assets/img/integration/apic-roks/Snip20190910_67.png)\n\n- Define the owner\n![ProvOrg](/assets/img/integration/apic-roks/Snip20190910_68.png)\n\n- After you submit the organization will appear on the list:\n![ProvOrg](/assets/img/integration/apic-roks/Snip20190910_69.png)\n\n- Navigate to the API Manager, in our case the endpoint was:\nhttps://mgmt.icp-proxy.icp4i-6550a99fb8cff23207ccecc2183787a9-0001.us-east.containers.appdomain.cloud/manage\n\n- Login as the owner (defined in the previous step), the API Manager page should open:\n![API Mgr](/assets/img/integration/apic-roks/Snip20190910_70.png)\n\n- You can navigate to the catalog:\n![Sandbox](/assets/img/integration/apic-roks/Snip20190910_71.png)\n\n- and create portal\n![Create portal](/assets/img/integration/apic-roks/Snip20190910_73.png)\n\n- You can also assign the gateway to the catalog\n![Catalog](/assets/img/integration/apic-roks/Snip20190910_79.png)\n\nWith that, your API Connect instance is ready for usage. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n---\n","fileAbsolutePath":"/home/travis/build/ibm-cloud-architecture/cloudpak8s/src/pages/integration/deploy-api-mgmt/index.mdx"}}}}