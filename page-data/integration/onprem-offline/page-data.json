{"componentChunkName":"component---src-pages-integration-onprem-offline-index-mdx","path":"/integration/onprem-offline/","result":{"pageContext":{"frontmatter":{"title":"onprem-offline","weight":400},"relativePagePath":"/integration/onprem-offline/index.mdx","titleType":"page","MdxNode":{"id":"df44a2b3-9e4c-5e1d-a222-7ac11258d191","children":[],"parent":"6cc8ac23-6c6a-5d5a-848c-416c62cf0f8d","internal":{"content":"---\ntitle: onprem-offline\nweight: 400\n---\n\n- [Introduction](#introduction)\n- [Prepare For Installation](#prepare-for-installation)\n- [Run the Integration Cloud Pak install](#run-the-integration-cloud-pak-install)\n- [Deploy Capabilities](#deploy-capabilities)\n- [Example files](#example-files)\n  - [config.yaml](#configyaml)\n\n## Introduction\n\nThis page describes all the steps on how to deploy the Integration Cloud Pak to a VMWARE onprem environment using the IBM Entitled registry. The steps below includes instructions to:\n1. Prepare the bastion node for installation\n2. Run the Integration Cloud Pak installer to deploy to an existing OpenShift cluster\n\n\n## Prepare For Installation\n\nIn many production scenarios the master nodes may not be accessed via ssh, we have to choose bastion node to proceed with the installation. \n\n**Installer Node requirements:**  \n- Sufficient resources of `4cpu 16GB ram ~120 GB Diskspace`\n- OpenShift CLI, which can be installed following the instruction [here on IBM Cloud](https://cloud.ibm.com/docs/openshift?topic=openshift-openshift-cli).\n- Install Docker (v2.2 is compliant and works with Open Shift)\n- Install Kubernetes CLI kubectl \n\nOnce the CLIs are installed, check if you can login to openshift environment:\n\n  1. Get login token from openshift console\n  2. Run the `oc login` command from a terminal shell.\n  3. You should see the cluster logged in message along with list of projects.\n  \n**Installing on Master or Infra node:**  \nThe value of the master, proxy, and management parameters is an array and can have multiple nodes. Due to a limitation from OpenShift, if you want to deploy IBM Cloud Private on any OpenShift master or infrastructure node, you must label the node as an OpenShift compute node with the following command:  \n```md\noc label node <master node host name/infrastructure node host name> node-role.kubernetes.io/compute=true\n```\n\n## Run the Integration Cloud Pak install\n\nIntegration Cloud Pak provides a single installer that installs ICP as well loads all the helm charts for integration capabilities. In this example CP4I will be installed on the master node.\n\n1. Download Integration Cloud Pak installer on the installer node. See [Pre-requisites](../pre-reqs) for guidance.\n     \n2. Open a command line window on the boot node, and extract the contents of the Cloud Pak. It is a general recommendation to create a directory in /opt and extract into that directory:  \n``` md \ntar xf ibm-cp-int-2019.4.1-online.tar.gz --directory /opt/cp4i\n```\n\nOnce untarred, you can navigate to the directory where the packages was untarred to and type `tree`.  It will look like the below\n\n![](1.untar-cp4i.png)\n\n3. Load the images onto your local docker registry:\n``` md\nsudo docker load -i installer_files/cluster/images/icp-inception-3.2.2.tgz\n```\n\n4. Change to the `installer_files/cluster/` directory. Place the cluster configuration files (kubeconfig) in the `installer_files/cluster/` directory. You can also use the following command after using oc login as admin.  Make sure your file only has one cluster context defined with in it, and that context is the location of your target cluster.\n``` md\noc config view > kubeconfig\n```\n\n4. Note down the IP addresses of OpenShift worker nodes. To get the IP addresses of the worker nodes, run:\n``` md\noc get nodes -o wide\n```  \n\n5. Navigate to your cluster directory `/opt/cp4i/installer_files/cluster`.  \n   \n6. Edit the config.yaml with the information you have collected above. See the example at the end of the page for guidance.\n\nHere are the fields to update with your respective values based on your environment:\n\n- under cluster_nodes heading -> set the hostnames for `Master`, `Proxy` and `Management`.  For non-prdoduction type system, setting and proxy to the same host is fine.  use the short name for your nodes (e.g. compute1, compute2 etc)\n- under storage_class -> choose your default storage class here - use `oc get sc` to get a list of available storageclasses.\n- docker_user -> `ekey`\n- docker_password -> set this to your entitlement key\n\ninstructions to get your entitlement key can be found [here](https://github.ibm.com/CloudPakOpenContent/cloudpak-entitlement) \n\n9. Run the installer with:\n  ``` \n  sudo docker run -t --net=host -e LICENSE=accept -v $(pwd):/installer/cluster:z -v /var/run:/var/run:z -v /etc/docker:/etc/docker:z --security-opt label:disable ibmcom/icp-inception-amd64:3.2.2 addon -vvv | tee install.log\n  ```\n9. If the namespaces for the different capabilities did not create you can create them manually using the scripts in `installer_files/cluster/resources` e.g. ace.yaml, apic.yaml.  Simply run each script using this syntax\n``` md\noc create -f <scriptname>.yaml\n```\n\n9. Once the process is complete, you will need to create your `ibm_entitlement_key` secrets in all of the main component namespaces.  You can accomplish this by running the `create_secrets.sh` script. Before doing so, export these two variables\n``` md\nexport DOCKER_REGISTRY_USER=ekey\nexport DOCKER_REGISTRY_PASS=<your entitlement key>\n```\n10. It will be helpful to understand what your proxy node address is, as it will be referenced several time when deploying the individual capabilities.  Run this command and take note of its output.\n``` md\noc get configmap -n kube-public ibmcloud-cluster-info -o=jsonpath=\"{.data.proxy_address}\"`\n```\n\n## Deploy Capabilities\n\nIt is recommended that you install the tracing capability first\n\n-  [Tracing](../deploy-tracing)\n-  [App Connect](../deploy-integration)\n-  [API Connect](../deploy-api-mgmt)\n-  [MQ](../deploy-queue-manager)\n-  [Event Streams](../deploy-eventstreams)\n-  [Aspera](../deploy-fast-file-transfer)\n-  [DataPower](../deploy-secure-gateway)\n-  [Asset Repository](../deploy-asset-repo)\n\n## Example files\n\nThis section contains examples of files you will be using throughout the installation. Refer to them for guidance on how to populate your own version of the files.\n\n\n### config.yaml\n\n```\n# Nodes selected to run common services components.\n#\n# The value of the master, proxy, and management parameters is an array,\n# by providing multiple nodes the common services will be configured in\n# a high availability configuration.\n#\n# It is recommended to install the components onto one or more openshift\n# worker nodes. The master, proxy, and management components can all share\n# the same node or set of nodes.\ncluster_nodes:\n  master:\n    - compute1\n  proxy:\n    - compute1\n  management:\n    - compute2\n# This storage class is used to store persistent data for the common services\n# components\nstorage_class: csi-cephfs\n## You can set a different storage class for storing log data.\n## By default it will use the value of storage_class.\n# elasticsearch_storage_class:\n# These settings enable the installer to install common services from the IBM\n# Entitled Registry. You will need to supply your entitlement key as described\n# at https://github.com/ibm/charts\nprivate_registry_enabled: true\nimage_repo: cp.icr.io/cp/icp-foundation\ndocker_username: ekey\ndocker_password: <your_entitlement_key>\ndefault_admin_password: admin\npassword_rules:\n  - '(.*)'\nmanagement_services:\n  # Common services\n  iam-policy-controller: enabled\n  metering: enabled\n  licensing: disabled\n  monitoring: enabled\n  nginx-ingress: enabled\n  common-web-ui: enabled\n  catalog-ui: enabled\n  mcm-kui: enabled\n  logging: enabled\n  audit-logging: disabled\n  system-healthcheck-service: disabled\n  multitenancy-enforcement: disabled\n  configmap-watcher: disabled\n# This section installs the IBM Cloud Pak for Integration Platform Navigator.\n# The navigator will be available after installation at:\n# https://ibm-icp4i-prod-integration.<openshift apps domain>/\narchive_addons:\n  icp4i:\n    namespace: integration\n    repo: local-charts\n    path: icp4icontent/IBM-Cloud-Pak-for-Integration-3.0.0.tgz\n    charts:\n      - name: ibm-icp4i-prod\n        values: {}\n```\n","type":"Mdx","contentDigest":"5e05c4496381fdd8c5308eb6a9ea2ba1","counter":238,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"onprem-offline","weight":400},"exports":{},"rawBody":"---\ntitle: onprem-offline\nweight: 400\n---\n\n- [Introduction](#introduction)\n- [Prepare For Installation](#prepare-for-installation)\n- [Run the Integration Cloud Pak install](#run-the-integration-cloud-pak-install)\n- [Deploy Capabilities](#deploy-capabilities)\n- [Example files](#example-files)\n  - [config.yaml](#configyaml)\n\n## Introduction\n\nThis page describes all the steps on how to deploy the Integration Cloud Pak to a VMWARE onprem environment using the IBM Entitled registry. The steps below includes instructions to:\n1. Prepare the bastion node for installation\n2. Run the Integration Cloud Pak installer to deploy to an existing OpenShift cluster\n\n\n## Prepare For Installation\n\nIn many production scenarios the master nodes may not be accessed via ssh, we have to choose bastion node to proceed with the installation. \n\n**Installer Node requirements:**  \n- Sufficient resources of `4cpu 16GB ram ~120 GB Diskspace`\n- OpenShift CLI, which can be installed following the instruction [here on IBM Cloud](https://cloud.ibm.com/docs/openshift?topic=openshift-openshift-cli).\n- Install Docker (v2.2 is compliant and works with Open Shift)\n- Install Kubernetes CLI kubectl \n\nOnce the CLIs are installed, check if you can login to openshift environment:\n\n  1. Get login token from openshift console\n  2. Run the `oc login` command from a terminal shell.\n  3. You should see the cluster logged in message along with list of projects.\n  \n**Installing on Master or Infra node:**  \nThe value of the master, proxy, and management parameters is an array and can have multiple nodes. Due to a limitation from OpenShift, if you want to deploy IBM Cloud Private on any OpenShift master or infrastructure node, you must label the node as an OpenShift compute node with the following command:  \n```md\noc label node <master node host name/infrastructure node host name> node-role.kubernetes.io/compute=true\n```\n\n## Run the Integration Cloud Pak install\n\nIntegration Cloud Pak provides a single installer that installs ICP as well loads all the helm charts for integration capabilities. In this example CP4I will be installed on the master node.\n\n1. Download Integration Cloud Pak installer on the installer node. See [Pre-requisites](../pre-reqs) for guidance.\n     \n2. Open a command line window on the boot node, and extract the contents of the Cloud Pak. It is a general recommendation to create a directory in /opt and extract into that directory:  \n``` md \ntar xf ibm-cp-int-2019.4.1-online.tar.gz --directory /opt/cp4i\n```\n\nOnce untarred, you can navigate to the directory where the packages was untarred to and type `tree`.  It will look like the below\n\n![](1.untar-cp4i.png)\n\n3. Load the images onto your local docker registry:\n``` md\nsudo docker load -i installer_files/cluster/images/icp-inception-3.2.2.tgz\n```\n\n4. Change to the `installer_files/cluster/` directory. Place the cluster configuration files (kubeconfig) in the `installer_files/cluster/` directory. You can also use the following command after using oc login as admin.  Make sure your file only has one cluster context defined with in it, and that context is the location of your target cluster.\n``` md\noc config view > kubeconfig\n```\n\n4. Note down the IP addresses of OpenShift worker nodes. To get the IP addresses of the worker nodes, run:\n``` md\noc get nodes -o wide\n```  \n\n5. Navigate to your cluster directory `/opt/cp4i/installer_files/cluster`.  \n   \n6. Edit the config.yaml with the information you have collected above. See the example at the end of the page for guidance.\n\nHere are the fields to update with your respective values based on your environment:\n\n- under cluster_nodes heading -> set the hostnames for `Master`, `Proxy` and `Management`.  For non-prdoduction type system, setting and proxy to the same host is fine.  use the short name for your nodes (e.g. compute1, compute2 etc)\n- under storage_class -> choose your default storage class here - use `oc get sc` to get a list of available storageclasses.\n- docker_user -> `ekey`\n- docker_password -> set this to your entitlement key\n\ninstructions to get your entitlement key can be found [here](https://github.ibm.com/CloudPakOpenContent/cloudpak-entitlement) \n\n9. Run the installer with:\n  ``` \n  sudo docker run -t --net=host -e LICENSE=accept -v $(pwd):/installer/cluster:z -v /var/run:/var/run:z -v /etc/docker:/etc/docker:z --security-opt label:disable ibmcom/icp-inception-amd64:3.2.2 addon -vvv | tee install.log\n  ```\n9. If the namespaces for the different capabilities did not create you can create them manually using the scripts in `installer_files/cluster/resources` e.g. ace.yaml, apic.yaml.  Simply run each script using this syntax\n``` md\noc create -f <scriptname>.yaml\n```\n\n9. Once the process is complete, you will need to create your `ibm_entitlement_key` secrets in all of the main component namespaces.  You can accomplish this by running the `create_secrets.sh` script. Before doing so, export these two variables\n``` md\nexport DOCKER_REGISTRY_USER=ekey\nexport DOCKER_REGISTRY_PASS=<your entitlement key>\n```\n10. It will be helpful to understand what your proxy node address is, as it will be referenced several time when deploying the individual capabilities.  Run this command and take note of its output.\n``` md\noc get configmap -n kube-public ibmcloud-cluster-info -o=jsonpath=\"{.data.proxy_address}\"`\n```\n\n## Deploy Capabilities\n\nIt is recommended that you install the tracing capability first\n\n-  [Tracing](../deploy-tracing)\n-  [App Connect](../deploy-integration)\n-  [API Connect](../deploy-api-mgmt)\n-  [MQ](../deploy-queue-manager)\n-  [Event Streams](../deploy-eventstreams)\n-  [Aspera](../deploy-fast-file-transfer)\n-  [DataPower](../deploy-secure-gateway)\n-  [Asset Repository](../deploy-asset-repo)\n\n## Example files\n\nThis section contains examples of files you will be using throughout the installation. Refer to them for guidance on how to populate your own version of the files.\n\n\n### config.yaml\n\n```\n# Nodes selected to run common services components.\n#\n# The value of the master, proxy, and management parameters is an array,\n# by providing multiple nodes the common services will be configured in\n# a high availability configuration.\n#\n# It is recommended to install the components onto one or more openshift\n# worker nodes. The master, proxy, and management components can all share\n# the same node or set of nodes.\ncluster_nodes:\n  master:\n    - compute1\n  proxy:\n    - compute1\n  management:\n    - compute2\n# This storage class is used to store persistent data for the common services\n# components\nstorage_class: csi-cephfs\n## You can set a different storage class for storing log data.\n## By default it will use the value of storage_class.\n# elasticsearch_storage_class:\n# These settings enable the installer to install common services from the IBM\n# Entitled Registry. You will need to supply your entitlement key as described\n# at https://github.com/ibm/charts\nprivate_registry_enabled: true\nimage_repo: cp.icr.io/cp/icp-foundation\ndocker_username: ekey\ndocker_password: <your_entitlement_key>\ndefault_admin_password: admin\npassword_rules:\n  - '(.*)'\nmanagement_services:\n  # Common services\n  iam-policy-controller: enabled\n  metering: enabled\n  licensing: disabled\n  monitoring: enabled\n  nginx-ingress: enabled\n  common-web-ui: enabled\n  catalog-ui: enabled\n  mcm-kui: enabled\n  logging: enabled\n  audit-logging: disabled\n  system-healthcheck-service: disabled\n  multitenancy-enforcement: disabled\n  configmap-watcher: disabled\n# This section installs the IBM Cloud Pak for Integration Platform Navigator.\n# The navigator will be available after installation at:\n# https://ibm-icp4i-prod-integration.<openshift apps domain>/\narchive_addons:\n  icp4i:\n    namespace: integration\n    repo: local-charts\n    path: icp4icontent/IBM-Cloud-Pak-for-Integration-3.0.0.tgz\n    charts:\n      - name: ibm-icp4i-prod\n        values: {}\n```\n","fileAbsolutePath":"/home/travis/build/ibm-cloud-architecture/cloudpak8s/src/pages/integration/onprem-offline/index.mdx"}}}}